# glu-gestures
This repository contains code and material for this paper:

* Giovanni Saponaro, Lorenzo Jamone, Alexandre Bernardino and Giampiero Salvi. Interactive Robot Learning of Gestures, Language and Affordances. International Workshop on Grounding Language Understanding (GLU), INTERSPEECH 2017. [[more information on ISCA Archive](http://www.isca-speech.org/archive/GLU_2017/abstracts/GLU2017_paper_20.html)] [GLU 2017 workshop website](http://www.speech.kth.se/glu2017/)

## Description

Below we show the experimental setup, consisting of an iCub humanoid robot and a human user performing a manipulation gesture on a shared table with different objects on top. The depth sensor in the top-left corner is used to extract human hand coordinates for gesture recognition. Depending on the gesture and on the target object, the resulting effect will differ.

<img src="article/2017-07_camera-ready/figures/human_tap.jpg" alt="Experimental setup." style="width: 800px;"/>

## License

Released under the terms of the GPL v3.0 or later. See the file LICENSE for details.
