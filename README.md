# glu-gestures
GLU 2017 paper about robot learning of gestures, language and affordances. Below we show the experimental setup, consisting of an iCub humanoid robot and a human user performing a manipulation gesture on a shared table with different objects on top. The depth sensor in the top-left corner is used to extract human hand coordinates for gesture recognition. Depending on the gesture and on the target object, the resulting effect will differ.

<img src="article/2017-07_camera-ready/figures/human_tap.jpg" alt="Experimental setup." style="width: 800px;"/>

If you use this work for your research, please cite the following publication:

* Giovanni Saponaro, Lorenzo Jamone, Alexandre Bernardino and Giampiero Salvi. Interactive Robot Learning of Gestures, Language and Affordances. International Workshop on Grounding Language Understanding (GLU), INTERSPEECH 2017.
